{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os,sys,inspect\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import heapq\n",
    "\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir) \n",
    "\n",
    "from load import *\n",
    "def eval_NDCG(true, pred):\n",
    "    top_k = pred\n",
    "\n",
    "    for i, item in enumerate(top_k, 1):\n",
    "        if item == true:\n",
    "            return 1 / np.log2(i+1)\n",
    "    return 0\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers, callbacks, layers, losses\n",
    "from tensorflow.keras.layers import Dense, Concatenate, Activation, Add, BatchNormalization, Dropout, Input, Embedding, Flatten, Multiply\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "os.environ['PYTHONHASHSEED']=str(SEED)\n",
    "random.seed(SEED)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        # 프로그램 시작시에 메모리 증가가 설정되어야만 합니다\n",
    "        print(e)\n",
    "        \n",
    "def mish(x):\n",
    "    return x*tf.math.tanh(tf.math.softplus(x))\n",
    "\n",
    "def leakyrelu(x, factor=0.2):\n",
    "    return tf.maximum(x, factor*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data('../data/ml-100k/u.data', threshold=3)\n",
    "df = df[df['rating']==1].reset_index(drop=True)\n",
    "tdf = pd.pivot_table(df, index='userId', values='rating', columns='movieId').fillna(0)\n",
    "\n",
    "# 10개 이상 평가한 유저만 포함 => 0이 나오는 문제가 발생하여\n",
    "cnt = tdf.sum(1)\n",
    "df = df[df['userId'].isin(np.where(cnt >= 10)[0])].reset_index(drop=True)\n",
    "tdf = pd.pivot_table(df, index='userId', values='rating', columns='movieId').fillna(0)\n",
    "tdf.iloc[:,:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = []\n",
    "for i in tdf.index:\n",
    "    test_idx += list(np.random.choice(df[df['userId']==i].index, 1))\n",
    "    \n",
    "train = df.iloc[list(set(df.index)-set(test_idx)),:]\n",
    "test = df.iloc[test_idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uid, iid in zip(train['userId'].values, train['movieId'].values):\n",
    "    tdf.loc[uid, iid] = 1\n",
    "train =  tdf.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = tf.shape(z_mean)[0]\n",
    "    dim = tf.shape(z_mean)[1]\n",
    "    epsilon = tf.random.normal(shape=(batch, dim), stddev=0.01)\n",
    "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultVAE(tf.keras.models.Model):\n",
    "    def __init__(self, input_dim, latent_dim, lamda=1e-4):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.anneal = 0.\n",
    "        \n",
    "        self.model = self.build()\n",
    "\n",
    "    def compile(self, optimizer, loss_fn=None):\n",
    "        super().compile()\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "        \n",
    "    def build(self):\n",
    "        self.encoder = self.build_encoder()\n",
    "        self.decoder = self.build_decoder()\n",
    "        \n",
    "        inputs = self.encoder.input\n",
    "        \n",
    "        mu, log_var = self.encoder(inputs)\n",
    "        h = sampling([mu, log_var])\n",
    "        \n",
    "        outputs = self.decoder(h)\n",
    "    \n",
    "        return Model(inputs, outputs)\n",
    "    \n",
    "    def build_encoder(self):\n",
    "        inputs = Input(shape = (self.input_dim, ))\n",
    "        h = Dropout(0.2)(inputs)\n",
    "        \n",
    "        mu = Dense(self.latent_dim)(h)\n",
    "        log_var = Dense(self.latent_dim)(h)\n",
    "        \n",
    "        return Model(inputs, [mu, log_var])\n",
    "    \n",
    "    def build_decoder(self):\n",
    "        inputs = Input(shape = (self.latent_dim, ))\n",
    "        \n",
    "        outputs = Dense(self.input_dim, activation='sigmoid')(inputs)\n",
    "\n",
    "        return Model(inputs, outputs)\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        x = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            mu, log_var = self.encoder(x)\n",
    "            pred = self.model(x)\n",
    "            \n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(0.5*(log_var + tf.exp(log_var) + tf.pow(mu, 2)-1), 1, keepdims=True))\n",
    "            ce_loss = -tf.reduce_mean(tf.reduce_sum(tf.nn.log_softmax(pred) * x, -1))\n",
    "            \n",
    "            loss = ce_loss + kl_loss*self.anneal\n",
    "            \n",
    "        grads = tape.gradient(loss, self.model.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_weights))\n",
    "        \n",
    "        return {'loss': loss}\n",
    "    \n",
    "    def predict(self, data):\n",
    "        mu, log_var = self.encoder(data)\n",
    "        return self.decoder(mu)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = tf.data.Dataset.from_tensor_slices(train.values.astype(np.float32))\n",
    "loader = loader.batch(8, drop_remainder=True).shuffle(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = MultVAE(train.shape[1], 200)\n",
    "model.compile(optimizer=tf.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anneal 값에 따라서 성능변화가 심함.. 고찰 필요\n",
    "class AnnealCallback(callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.anneal_cap = 0.3\n",
    "        \n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        self.model.anneal =  min(self.anneal_cap, self.model.anneal+1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 413.5160\n",
      "Epoch 2/25\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 408.8344\n",
      "Epoch 3/25\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 403.9771\n",
      "Epoch 4/25\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 399.3782\n",
      "Epoch 5/25\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 400.7264\n",
      "Epoch 6/25\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 399.8045\n",
      "Epoch 7/25\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 398.2963\n",
      "Epoch 8/25\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 399.7597\n",
      "Epoch 9/25\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 397.7665\n",
      "Epoch 10/25\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 393.5316\n",
      "Epoch 11/25\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 393.7732\n",
      "Epoch 12/25\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 393.8577\n",
      "Epoch 13/25\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 394.1134\n",
      "Epoch 14/25\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 392.9901\n",
      "Epoch 15/25\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 392.2649\n",
      "Epoch 16/25\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 389.0090\n",
      "Epoch 17/25\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 388.3984\n",
      "Epoch 18/25\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 388.0277\n",
      "Epoch 19/25\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 387.9869\n",
      "Epoch 20/25\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 387.4396\n",
      "Epoch 21/25\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 386.1512\n",
      "Epoch 22/25\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 389.0087\n",
      "Epoch 23/25\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 386.3604\n",
      "Epoch 24/25\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 385.7380\n",
      "Epoch 25/25\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 386.3530\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d51b4c1d08>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(loader,\n",
    "         epochs=25,\n",
    "         callbacks=[AnnealCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "896it [01:53,  7.90it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3934388168754929"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k = 10\n",
    "\n",
    "scores = []\n",
    "for idx, i in tqdm(enumerate(train.index)):\n",
    "    item_to_pred = {item: pred for item, pred in zip(train.columns, model.model.predict(train.values)[idx])}\n",
    "    test_ = test[(test['userId']==i) & (test['rating']==1)]['movieId'].values\n",
    "    items = list(np.random.choice(list(filter(lambda x: x not in np.argwhere(train.values[idx]).flatten(), item_to_pred.keys())), 100)) + list(test_)\n",
    "    top_k_items = heapq.nlargest(top_k, items, key=item_to_pred.get)\n",
    "    \n",
    "    score = eval_NDCG(test_, top_k_items)\n",
    "    scores.append(score)\n",
    "    \n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "896it [04:22,  3.41it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4001024033054037"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k = 10\n",
    "\n",
    "scores = []\n",
    "for idx, i in tqdm(enumerate(train.index)):\n",
    "    item_to_pred = {item: pred for item, pred in zip(train.columns, model.predict(train.values)[idx])}\n",
    "    test_ = test[(test['userId']==i) & (test['rating']==1)]['movieId'].values\n",
    "    items = list(np.random.choice(list(filter(lambda x: x not in np.argwhere(train.values[idx]).flatten(), item_to_pred.keys())), 100)) + list(test_)\n",
    "    top_k_items = heapq.nlargest(top_k, items, key=item_to_pred.get)\n",
    "    \n",
    "    score = eval_NDCG(test_, top_k_items)\n",
    "    scores.append(score)\n",
    "    \n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
