{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os,sys,inspect\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import heapq\n",
    "\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir) \n",
    "\n",
    "from load import *\n",
    "def eval_NDCG(true, pred):\n",
    "    top_k = pred\n",
    "\n",
    "    for i, item in enumerate(top_k, 1):\n",
    "        if item == true:\n",
    "            return 1 / np.log2(i+1)\n",
    "    return 0\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers, callbacks, layers, losses\n",
    "from tensorflow.keras.layers import Dense, Concatenate, Activation, Add, BatchNormalization, Dropout, Input, Embedding, Flatten, Multiply\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "os.environ['PYTHONHASHSEED']=str(SEED)\n",
    "random.seed(SEED)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        # 프로그램 시작시에 메모리 증가가 설정되어야만 합니다\n",
    "        print(e)\n",
    "        \n",
    "def mish(x):\n",
    "    return x*tf.math.tanh(tf.math.softplus(x))\n",
    "\n",
    "def leakyrelu(x, factor=0.2):\n",
    "    return tf.maximum(x, factor*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data('../data/ml-100k/u.data', threshold=3)\n",
    "df = df[df['rating']==1].reset_index(drop=True)\n",
    "tdf = pd.pivot_table(df, index='userId', values='rating', columns='movieId').fillna(0)\n",
    "\n",
    "# 10개 이상 평가한 유저만 포함 => 0이 나오는 문제가 발생하여\n",
    "cnt = tdf.sum(1)\n",
    "df = df[df['userId'].isin(np.where(cnt >= 10)[0])].reset_index(drop=True)\n",
    "tdf = pd.pivot_table(df, index='userId', values='rating', columns='movieId').fillna(0)\n",
    "tdf.iloc[:,:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = []\n",
    "for i in tdf.index:\n",
    "    test_idx += list(np.random.choice(df[df['userId']==i].index, 1))\n",
    "    \n",
    "train = df.iloc[list(set(df.index)-set(test_idx)),:]\n",
    "test = df.iloc[test_idx, :]\n",
    "\n",
    "for uid, iid in zip(train['userId'].values, train['movieId'].values):\n",
    "    tdf.loc[uid, iid] = 1\n",
    "train =  tdf.copy().astype(np.float32)\n",
    "\n",
    "loader = tf.data.Dataset.from_tensor_slices(train.values.astype(np.float32))\n",
    "loader = loader.batch(8, drop_remainder=True).shuffle(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_norm_pdf(x, mu, logvar):\n",
    "    return -0.5*(logvar + tf.math.log(2 * np.pi) + tf.pow((x - mu), 2) / tf.exp(logvar))\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = tf.shape(z_mean)[0]\n",
    "    dim = tf.shape(z_mean)[1]\n",
    "    epsilon = tf.random.normal(shape=(batch, dim), stddev=0.01)\n",
    "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompositePrior(tf.keras.models.Model):\n",
    "    def __init__(self, x_dim, latent_dim, mixture_weights = [3/20, 15/20, 2/20]):\n",
    "        super().__init__()\n",
    "        self.encoder_old = Encoder(x_dim, latent_dim, dropout_rate=0)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.mixture_weights = mixture_weights\n",
    "        \n",
    "        self.mu_prior = self.add_weight(shape=(self.latent_dim, ), initializer = tf.zeros_initializer(), trainable=False)\n",
    "        self.logvar_prior  = self.add_weight(shape=(self.latent_dim, ), initializer = tf.zeros_initializer(), trainable=False)\n",
    "        self.logvar_unif_prior = self.add_weight(shape=(self.latent_dim, ), initializer = tf.constant_initializer(10), trainable=False)\n",
    "        \n",
    "    def call(self, x, z):\n",
    "        post_mu, post_logvar = self.encoder_old(x)\n",
    "        \n",
    "        stnd_prior = log_norm_pdf(z, self.mu_prior, self.logvar_prior)\n",
    "        post_prior = log_norm_pdf(z, post_mu, post_logvar)\n",
    "        unif_prior = log_norm_pdf(z, self.mu_prior, self.logvar_unif_prior)\n",
    "        \n",
    "        gaussians = [stnd_prior, post_prior, unif_prior]\n",
    "        gaussians = [g+tf.math.log(w) for g, w in zip(gaussians, self.mixture_weights)]\n",
    "        \n",
    "        density = tf.stack(gaussians, -1)\n",
    "        return tf.math.log(tf.reduce_sum(tf.exp(density), -1)) # logsumexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.models.Model):\n",
    "    def __init__(self, x_dim, latent_dim, dropout_rate = 0.1):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.x_dim = x_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.model = self.build_model()\n",
    "        \n",
    "    def build_model(self): # now just shallow net\n",
    "        x_in = Input(shape=(self.x_dim, ))\n",
    "        \n",
    "        h = Dense(1024, activation='relu')(x_in)\n",
    "        mu = Dense(self.latent_dim)(h)\n",
    "        logvar = Dense(self.latent_dim)(h)\n",
    "        \n",
    "        return Model(x_in, [mu, logvar])\n",
    "        \n",
    "    def call(self, x):\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.pow(x, 2), -1, keepdims=True))\n",
    "        x = x/norm\n",
    "        if self.dropout_rate>0:\n",
    "            x = Dropout(self.dropout_rate)(x)\n",
    "        \n",
    "        return self.model(x)\n",
    "\n",
    "class RecVAE(tf.keras.models.Model):\n",
    "    def __init__(self, x_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder(x_dim, latent_dim)\n",
    "        self.decoder = Dense(x_dim)\n",
    "        self.prior = CompositePrior(x_dim, latent_dim)\n",
    "        \n",
    "    def call(self, data):\n",
    "        mu, logvar = self.encoder(data)\n",
    "        z = sampling([mu, logvar])\n",
    "        recon = self.decoder(z)\n",
    "        \n",
    "        return mu, logvar, z, recon\n",
    "    \n",
    "    def predict(self, data):\n",
    "        mu, logvar = self.encoder(data)\n",
    "        z = sampling([mu, logvar])\n",
    "        recon = self.decoder(z)\n",
    "        \n",
    "        return recon\n",
    "    \n",
    "    def update_prior(self):\n",
    "        self.prior.encoder_old.set_weights(self.encoder.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_train(model, loader, optimizer, target, gamma=1.):\n",
    "    total_loss = 0.\n",
    "    for x in loader:\n",
    "        norm = tf.reduce_sum(x, -1, keepdims=True)\n",
    "        kl_weight = gamma*norm\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            mu, logvar, z, pred = model(x)\n",
    "            \n",
    "#             kl_loss = tf.reduce_mean(tf.reduce_sum(0.5*(logvar + tf.exp(logvar) + tf.pow(mu, 2)-1), 1, keepdims=True))\n",
    "            kl_loss = tf.reduce_mean(log_norm_pdf(z, mu, logvar) - tf.multiply(model.prior(x, z), kl_weight))\n",
    "            ce_loss = -tf.reduce_mean(tf.reduce_sum(tf.nn.log_softmax(pred) * x, -1))\n",
    "            \n",
    "            loss = ce_loss + kl_loss*kl_weight\n",
    "            \n",
    "        if target == 'encoder':\n",
    "            grads = tape.gradient(loss, model.encoder.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, model.encoder.trainable_weights))\n",
    "        else:\n",
    "            grads = tape.gradient(loss, model.decoder.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, model.decoder.trainable_weights))\n",
    "            \n",
    "        total_loss += tf.reduce_sum(loss)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "\n",
    "model = RecVAE(train.shape[1], 200)\n",
    "enc_opt = optimizers.Adam()\n",
    "dec_opt = optimizers.Adam()\n",
    "\n",
    "for e in range(epochs):\n",
    "    # alternating \n",
    "    ## train step\n",
    "    tf_train(model, loader, enc_opt, 'encoder')\n",
    "    model.update_prior()\n",
    "    tf_train(model, loader, dec_opt, 'decoder')\n",
    "    ## eval step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "896it [05:59,  2.49it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.41734477604306824"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k = 10\n",
    "\n",
    "scores = []\n",
    "for idx, i in tqdm(enumerate(train.index)):\n",
    "    item_to_pred = {item: pred.numpy() for item, pred in zip(train.columns, model.predict(train.values)[idx])}\n",
    "    test_ = test[(test['userId']==i) & (test['rating']==1)]['movieId'].values\n",
    "    items = list(np.random.choice(list(filter(lambda x: x not in np.argwhere(train.values[idx]).flatten(), item_to_pred.keys())), 100)) + list(test_)\n",
    "    top_k_items = heapq.nlargest(top_k, items, key=item_to_pred.get)\n",
    "    \n",
    "    score = eval_NDCG(test_, top_k_items)\n",
    "    scores.append(score)\n",
    "#     break\n",
    "np.mean(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
