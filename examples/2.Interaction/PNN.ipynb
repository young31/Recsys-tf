{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os,sys,inspect\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir) \n",
    "\n",
    "from load import *\n",
    "from evals import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers, callbacks, layers, losses\n",
    "from tensorflow.keras.layers import Dense, Concatenate, Activation, Add, BatchNormalization, Dropout, Input, Embedding, Flatten, Multiply\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "os.environ['PYTHONHASHSEED']=str(SEED)\n",
    "random.seed(SEED)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        # 프로그램 시작시에 메모리 증가가 설정되어야만 합니다\n",
    "        print(e)\n",
    "        \n",
    "def mish(x):\n",
    "    return x*tf.math.tanh(tf.math.softplus(x))\n",
    "\n",
    "def leakyrelu(x, factor=0.2):\n",
    "    return tf.maximum(x, factor*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data('../data/ml-100k/u.data', threshold=3)\n",
    "uuid = df['userId'].unique()\n",
    "uiid = df['movieId'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.15, random_state=SEED, stratify=df['userId'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((85000, 2), (15000, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_X = np.stack([train['userId'].values.astype(np.int32), train['movieId'].values.astype(np.int32)], 1)\n",
    "test_X = np.stack([test['userId'].values.astype(np.int32), test['movieId'].values.astype(np.int32)], 1)\n",
    "\n",
    "tr_X.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InnerProduct(layers.Layer):\n",
    "    def __init__(self, x_dims):\n",
    "        super().__init__()\n",
    "        self.x_dims = x_dims\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        n = len(self.x_dims)\n",
    "        \n",
    "        p = []\n",
    "        q = []\n",
    "        for i in range(n):\n",
    "            for j in range(i+1, n):\n",
    "                p.append(i)\n",
    "                q.append(j)\n",
    "                \n",
    "        p = tf.gather(inputs, p, axis=1)\n",
    "        q = tf.gather(inputs, q, axis=1)\n",
    "        \n",
    "        out = p*q\n",
    "        out = tf.squeeze(out, 1)\n",
    "#         out = tf.reduce_sum(out, axis=2)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class OuterProduct(layers.Layer):\n",
    "    def __init__(self, x_dims, kernel_type='mat'):\n",
    "        super().__init__()\n",
    "        self.x_dims = x_dims\n",
    "        self.kernel_type = kernel_type\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        n, m, k = input_shape\n",
    "        \n",
    "        if self.kernel_type == 'mat':\n",
    "            self.kernel = self.add_weight(shape=(k, (m*(m-1)//2), k), \n",
    "                                         initializer = tf.zeros_initializer())\n",
    "        else:\n",
    "            self.kernel = self.add_weight(shape=((m*(m-1)//2), k),\n",
    "                                         initializer = tf.zeros_initializer())\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        n = len(self.x_dims)\n",
    "        \n",
    "        p = []\n",
    "        q = []\n",
    "        for i in range(n):\n",
    "            for j in range(i+1, n):\n",
    "                p.append(i)\n",
    "                q.append(j)\n",
    "                \n",
    "        p = tf.gather(inputs, p, axis=1)\n",
    "        q = tf.gather(inputs, q, axis=1)\n",
    "        \n",
    "        if self.kernel_type == 'mat':\n",
    "            kp = tf.transpose(tf.reduce_sum(tf.expand_dims(p, 1) * self.kernel, -1), [0, 2, 1])\n",
    "            out = tf.reduce_sum(kp * q, -1)\n",
    "        else:\n",
    "            out = tf.reduce_sum(p * q * tf.expand_dims(self.kernel, 0), -1)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PNN(Model):\n",
    "    def __init__(self, x_dims, latent_dim, dnn_layers, model_type='inner', l2_emb=1e-4):\n",
    "        super().__init__()\n",
    "        self.x_dims = x_dims\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.embedding = Embedding(sum(x_dims)+1, latent_dim, input_length=1, embeddings_regularizer=l2(l2_emb))\n",
    "\n",
    "        self.linear = Dense(latent_dim)\n",
    "\n",
    "        if model_type == 'inner':\n",
    "            self.pnn = InnerProduct(x_dims)\n",
    "        elif model_type == 'outer':\n",
    "            self.pnn = OuterProduct(x_dims)\n",
    "        else:\n",
    "            raise ValueError('no available model type')\n",
    "        \n",
    "        self.dnn = [Dense(unit, activation='relu') for unit in dnn_layers]\n",
    "        \n",
    "        self.final = Dense(1)\n",
    "        \n",
    "        self.flatten = Flatten()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        emb = self.embedding(inputs + tf.constant((0, *np.cumsum(self.x_dims)))[:-1])\n",
    "        \n",
    "        linear = self.flatten(self.linear(emb))\n",
    "        quadratic = self.pnn(emb)\n",
    "\n",
    "        concat = tf.concat([linear, quadratic], -1)\n",
    "        \n",
    "        out = concat\n",
    "        for layer in self.dnn:\n",
    "            out = layer(out)\n",
    "        \n",
    "        out = self.final(out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipnn = PNN((len(uuid), len(uiid)), 8, [64, 32])\n",
    "opnn = PNN((len(uuid), len(uiid)), 8, [64, 32], 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2391/2391 [==============================] - 8s 4ms/step - loss: 0.5956 - val_loss: 0.5767\n",
      "Epoch 2/10\n",
      "2391/2391 [==============================] - 8s 3ms/step - loss: 0.5608 - val_loss: 0.5746\n",
      "Epoch 3/10\n",
      "2391/2391 [==============================] - 8s 3ms/step - loss: 0.5534 - val_loss: 0.5728\n",
      "Epoch 4/10\n",
      "2391/2391 [==============================] - 8s 3ms/step - loss: 0.5472 - val_loss: 0.5703\n",
      "Epoch 5/10\n",
      "2391/2391 [==============================] - 8s 4ms/step - loss: 0.5408 - val_loss: 0.5708\n",
      "Epoch 6/10\n",
      "2391/2391 [==============================] - 9s 4ms/step - loss: 0.5322 - val_loss: 0.5752\n",
      "Epoch 7/10\n",
      "2391/2391 [==============================] - 9s 4ms/step - loss: 0.5251 - val_loss: 0.5807\n",
      "Epoch 8/10\n",
      "2391/2391 [==============================] - 9s 4ms/step - loss: 0.5186 - val_loss: 0.5778\n",
      "Epoch 9/10\n",
      "2391/2391 [==============================] - 8s 3ms/step - loss: 0.5134 - val_loss: 0.5824\n",
      "Epoch 10/10\n",
      "2391/2391 [==============================] - 8s 3ms/step - loss: 0.5076 - val_loss: 0.5860\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x213c6bc40c8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipnn.compile(loss=losses.BinaryCrossentropy(from_logits=True), \n",
    "            optimizer=optimizers.Adam())\n",
    "\n",
    "ipnn.fit(tr_X, \n",
    "       train['rating'].values,\n",
    "      epochs=10,\n",
    "      shuffle=True,\n",
    "      validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2391/2391 [==============================] - 9s 4ms/step - loss: 0.5968 - val_loss: 0.5802\n",
      "Epoch 2/10\n",
      "2391/2391 [==============================] - 9s 4ms/step - loss: 0.5648 - val_loss: 0.5759\n",
      "Epoch 3/10\n",
      "2391/2391 [==============================] - 10s 4ms/step - loss: 0.5552 - val_loss: 0.5743\n",
      "Epoch 4/10\n",
      "2391/2391 [==============================] - 10s 4ms/step - loss: 0.5503 - val_loss: 0.5706\n",
      "Epoch 5/10\n",
      "2391/2391 [==============================] - 9s 4ms/step - loss: 0.5465 - val_loss: 0.5715\n",
      "Epoch 6/10\n",
      "2391/2391 [==============================] - 9s 4ms/step - loss: 0.5406 - val_loss: 0.5745\n",
      "Epoch 7/10\n",
      "2391/2391 [==============================] - 9s 4ms/step - loss: 0.5342 - val_loss: 0.5772\n",
      "Epoch 8/10\n",
      "2391/2391 [==============================] - 9s 4ms/step - loss: 0.5279 - val_loss: 0.5753\n",
      "Epoch 9/10\n",
      "2391/2391 [==============================] - 9s 4ms/step - loss: 0.5223 - val_loss: 0.5786\n",
      "Epoch 10/10\n",
      "2391/2391 [==============================] - 9s 4ms/step - loss: 0.5169 - val_loss: 0.5830\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x213dd9a4988>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opnn.compile(loss=losses.BinaryCrossentropy(from_logits=True), \n",
    "            optimizer=optimizers.Adam())\n",
    "\n",
    "opnn.fit(tr_X, \n",
    "       train['rating'].values,\n",
    "      epochs=10,\n",
    "      shuffle=True,\n",
    "      validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_i = ipnn.predict(test_X)\n",
    "pred_o = opnn.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7194666666666667\n",
      "0.7258\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(np.where(pred_i>0., 1, 0).flatten() == test['rating'].values) / len(pred_i))\n",
    "print(np.sum(np.where(pred_o>0., 1, 0).flatten() == test['rating'].values) / len(pred_o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7878999244272435\n",
      "0.7308988764044944\n",
      "0.7820389516710747\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score,  roc_auc_score, precision_recall_fscore_support\n",
    "\n",
    "# inner\n",
    "print(roc_auc_score(test['rating'].values, pred_i))\n",
    "print(precision_score(test['rating'].values, np.where(pred_i>0., 1, 0)))\n",
    "print(recall_score(test['rating'].values, np.where(pred_i>0., 1, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7907344065609905\n",
      "0.7306637410861218\n",
      "0.8006732387593172\n"
     ]
    }
   ],
   "source": [
    "# outer\n",
    "print(roc_auc_score(test['rating'].values, pred_o))\n",
    "print(precision_score(test['rating'].values, np.where(pred_o>0., 1, 0)))\n",
    "print(recall_score(test['rating'].values, np.where(pred_o>0., 1, 0)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
