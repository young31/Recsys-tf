{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/microsoft/recommenders/blob/main/examples/00_quick_start/wide_deep_movielens.ipynb\n",
    "# not implement cross column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os,sys,inspect\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir) \n",
    "\n",
    "from load import *\n",
    "from evals import *\n",
    "from models import WideAndDeep\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers, callbacks, layers, losses\n",
    "from tensorflow.keras.layers import Dense, Concatenate, Activation, Add, BatchNormalization, Dropout, Input, Embedding, Flatten, Multiply\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "os.environ['PYTHONHASHSEED']=str(SEED)\n",
    "random.seed(SEED)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        # 프로그램 시작시에 메모리 증가가 설정되어야만 합니다\n",
    "        print(e)\n",
    "        \n",
    "def mish(x):\n",
    "    return x*tf.math.tanh(tf.math.softplus(x))\n",
    "\n",
    "def leakyrelu(x, factor=0.2):\n",
    "    return tf.maximum(x, factor*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data('../data/ml-100k/u.data', threshold=3)\n",
    "\n",
    "uuid = df['userId'].unique()\n",
    "uiid = df['movieId'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.15, random_state=SEED, stratify=df['userId'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeep(keras.Model):\n",
    "    def __init__(self, u_dim, i_dim, u_emb_dim=4, i_emb_dim=4):\n",
    "        super(WideAndDeep, self).__init__()\n",
    "        \n",
    "        self.u_dim = u_dim\n",
    "        self.i_dim = i_dim\n",
    "        self.u_emb_dim = u_emb_dim\n",
    "        self.i_emb_dim = i_emb_dim\n",
    "        \n",
    "        self.deep_model = self.build_deep_model()\n",
    "        self.wide_model = self.build_wide_model()\n",
    "\n",
    "\n",
    "    def compile(self, wide_optim, deep_optim, loss_fn):\n",
    "        super(WideAndDeep, self).compile()\n",
    "        self.wide_optim = wide_optim\n",
    "        self.deep_optim = deep_optim\n",
    "        self.loss_fn = loss_fn\n",
    "    \n",
    "    def build_deep_model(self):\n",
    "        u_input = Input(shape=(1, ))\n",
    "        i_input = Input(shape=(1, ))\n",
    "\n",
    "        u_emb = Flatten()(Embedding(self.u_dim, self.u_emb_dim, input_length=u_input.shape[1])(u_input))\n",
    "        i_emb = Flatten()(Embedding(self.i_dim, self.i_emb_dim, input_length=i_input.shape[1])(i_input))\n",
    "\n",
    "        concat = Concatenate()([u_emb, i_emb])\n",
    "        \n",
    "        h = Dense(256, activation='relu')(concat)\n",
    "        h = Dense(128, activation='relu')(h)\n",
    "        h = Dense(64, activation='relu')(h)\n",
    "        h = Dropout(0.2)(h)\n",
    "\n",
    "        out = Dense(1)(h)\n",
    "        \n",
    "        return Model([u_input, i_input], out, name='DeepModel')\n",
    "    \n",
    "    def build_wide_model(self):\n",
    "        u_input = Input(shape=(self.u_dim, ))\n",
    "        i_input = Input(shape=(self.i_dim, ))\n",
    "\n",
    "        concat = Concatenate()([u_input, i_input])\n",
    "        \n",
    "        out = Dense(1)(concat)\n",
    "        \n",
    "        return Model([u_input, i_input], out, name='WideModel')\n",
    "        \n",
    "    \n",
    "    def train_step(self, data):\n",
    "        X, y = data\n",
    "        user, item, user_ohe, item_ohe = X\n",
    "        \n",
    "        with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
    "            wide_logit = self.wide_model([user_ohe, item_ohe])\n",
    "            deep_logit = self.deep_model([user, item])\n",
    "            logit = 0.5*(wide_logit + deep_logit)\n",
    "            \n",
    "            loss = self.loss_fn(y, logit)\n",
    "            \n",
    "        wide_grads = tape1.gradient(loss, self.wide_model.trainable_weights)\n",
    "        self.wide_optim.apply_gradients(zip(wide_grads, self.wide_model.trainable_weights))\n",
    "        \n",
    "        deep_grads = tape2.gradient(loss, self.deep_model.trainable_weights)\n",
    "        self.deep_optim.apply_gradients(zip(deep_grads, self.deep_model.trainable_weights))\n",
    "        \n",
    "        return {'loss': loss}\n",
    "    \n",
    "    def call(self, data):\n",
    "        user, item, user_ohe, item_ohe = data\n",
    "        wide_logit = self.wide_model([user_ohe, item_ohe])\n",
    "        deep_logit = self.deep_model([user, item])\n",
    "        return 0.5*(wide_logit + deep_logit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2391/2391 [==============================] - 9s 4ms/step - loss: 0.5965 - val_loss: 0.0000e+00\n",
      "Epoch 2/10\n",
      "2391/2391 [==============================] - 9s 4ms/step - loss: 0.5550 - val_loss: 0.0000e+00\n",
      "Epoch 3/10\n",
      "2391/2391 [==============================] - 9s 4ms/step - loss: 0.5440 - val_loss: 0.0000e+00\n",
      "Epoch 4/10\n",
      "2391/2391 [==============================] - 9s 4ms/step - loss: 0.5373 - val_loss: 0.0000e+00\n",
      "Epoch 5/10\n",
      "2391/2391 [==============================] - 9s 4ms/step - loss: 0.5324 - val_loss: 0.0000e+00\n",
      "Epoch 6/10\n",
      "2391/2391 [==============================] - 9s 4ms/step - loss: 0.5253 - val_loss: 0.0000e+00\n",
      "Epoch 7/10\n",
      "2391/2391 [==============================] - 9s 4ms/step - loss: 0.5189 - val_loss: 0.0000e+00\n",
      "Epoch 8/10\n",
      "2391/2391 [==============================] - 9s 4ms/step - loss: 0.5125 - val_loss: 0.0000e+00\n",
      "Epoch 9/10\n",
      "2391/2391 [==============================] - 10s 4ms/step - loss: 0.5060 - val_loss: 0.0000e+00\n",
      "Epoch 10/10\n",
      "2391/2391 [==============================] - 9s 4ms/step - loss: 0.5005 - val_loss: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "wnd = WideAndDeep(len(uuid), len(uiid))\n",
    "wnd.compile(\n",
    "    optimizers.Adam(1e-3),\n",
    "    optimizers.Adam(1e-3),\n",
    "    losses.BinaryCrossentropy(from_logits=True)\n",
    "           )\n",
    "\n",
    "hist = wnd.fit([train['userId'].values, train['movieId'].values, to_categorical(train['userId']), to_categorical(train['movieId'])],\n",
    "                   train['rating'].values,\n",
    "                   shuffle=True,\n",
    "                   epochs=10,\n",
    "                   validation_split=0.1\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = wnd.predict([test['userId'].values, test['movieId'].values, to_categorical(test['userId'], len(uuid)), to_categorical(test['movieId'], len(uiid))])\n",
    "# np.mean(np.square(test['rating'].values, pred.flatten()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7208666666666667"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy\n",
    "np.sum(np.where(pred>0, 1, 0).flatten() == test['rating'].values) / len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7877736309877519\n",
      "0.7322091062394603\n",
      "0.7830007213272421\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score,  roc_auc_score\n",
    "\n",
    "print(roc_auc_score(test['rating'].values, pred.flatten()))\n",
    "print(precision_score(test['rating'].values, np.where(pred>0, 1, 0).flatten()))\n",
    "print(recall_score(test['rating'].values, np.where(pred>0, 1, 0).flatten()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
