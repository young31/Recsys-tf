{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://github.com/MogicianXD/CML_torch/tree/997690984989d41cef21fde0731b8bf0f8d96064"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "## multiple neg items ~ ranking loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os,sys,inspect\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import heapq\n",
    "\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir) \n",
    "\n",
    "from load import *\n",
    "def eval_NDCG(true, pred):\n",
    "    top_k = pred\n",
    "\n",
    "    for i, item in enumerate(top_k, 1):\n",
    "        if item == true:\n",
    "            return 1 / np.log2(i+1)\n",
    "    return 0\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers, callbacks, layers, losses, models\n",
    "from tensorflow.keras.layers import Dense, Concatenate, Activation, Add, BatchNormalization, Dropout, Input, Embedding, Flatten, Multiply\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "os.environ['PYTHONHASHSEED']=str(SEED)\n",
    "random.seed(SEED)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        # 프로그램 시작시에 메모리 증가가 설정되어야만 합니다\n",
    "        print(e)\n",
    "        \n",
    "def mish(x):\n",
    "    return x*tf.math.tanh(tf.math.softplus(x))\n",
    "\n",
    "def leakyrelu(x, factor=0.2):\n",
    "    return tf.maximum(x, factor*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data('../data/ml-100k/u.data', threshold=3)\n",
    "df = df[df['rating']==1].reset_index(drop=True)\n",
    "tdf = pd.pivot_table(df, index='userId', values='rating', columns='movieId').fillna(0)\n",
    "\n",
    "cnt = tdf.sum(1)\n",
    "df = df[df['userId'].isin(np.where(cnt >= 10)[0])].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = []\n",
    "tdf = pd.pivot_table(df, index='userId', values='rating', columns='movieId').fillna(0)\n",
    "for i in tdf.index:\n",
    "    test_idx += list(np.random.choice(df[df['userId']==i].index, 1))\n",
    "    \n",
    "train = df.iloc[list(set(df.index)-set(test_idx)),:]\n",
    "test = df.iloc[test_idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_trpilet(df, uiid):\n",
    "    uid_map = {}\n",
    "    for user in df['userId'].unique():\n",
    "        uid_map[user] = list(set(uiid) - set(df[df['userId']==user]['movieId'].unique()))\n",
    "\n",
    "    negs = []\n",
    "    for i in tqdm(range(len(df))):\n",
    "        user = df.values[i][0]\n",
    "        valid_negs = uid_map[user]\n",
    "        negs.append(np.random.choice(list(valid_negs)))\n",
    "        \n",
    "    df['neg'] = negs\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CML(models.Model):\n",
    "    def __init__(self, n_users, n_items, emb_dim, feature_shape=None):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.feature_shape=feature_shape\n",
    "        self.margin = 1.\n",
    "#         self.use_rank_weight = True\n",
    "        self.use_cov_loss = False\n",
    "        \n",
    "        # reg weights\n",
    "        self.feature_l2_reg = 0.1\n",
    "        self.feature_projection_scaling_factor = 0.5\n",
    "        self.cov_loss_weight = 0.1\n",
    "        \n",
    "        self.clip_norm = 1.\n",
    "        \n",
    "        self.user_embedding = Embedding(n_users, emb_dim)\n",
    "        self.item_embedding = Embedding(n_items, emb_dim)\n",
    "        \n",
    "        if self.feature_shape is not None:\n",
    "            self.mlp = Sequential([\n",
    "                Dense(self.feature_shape[0], activation='relu'),\n",
    "                Dense(emb_dim)\n",
    "            ])\n",
    "            \n",
    "    def call(self, inputs):\n",
    "        user = inputs[:,0]\n",
    "        item = inputs[:,1]\n",
    "        \n",
    "        user_emb = self.user_embedding(user)\n",
    "        item_emb = self.item_embedding(item)\n",
    "        \n",
    "        return -tf.reduce_sum(\n",
    "            tf.square(user_emb-item_emb), 1\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def train_step(self, inputs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.get_loss(inputs)\n",
    "        \n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        return {'loss': loss}\n",
    "            \n",
    "            \n",
    "    def get_loss(self, inputs):\n",
    "        X = inputs\n",
    "        loss = self._embedding_loss(X)\n",
    "        if self.use_cov_loss:\n",
    "            loss += self._covariance_loss()\n",
    "        return loss\n",
    "    \n",
    "    def _embedding_loss(self, inputs):\n",
    "        X = inputs\n",
    "        users = self.user_embedding(X[:, 0])\n",
    "\n",
    "        pos_items = self.item_embedding(X[:, 1])\n",
    "        neg_items = self.item_embedding(X[:, 2])\n",
    "        \n",
    "        pos_distances = tf.reduce_sum((users - pos_items) ** 2, 1)\n",
    "        distance_to_neg_items = tf.reduce_sum((users - neg_items) ** 2, 1)\n",
    "\n",
    "        # best negative item (among W negative samples) their distance to the user embedding (N)\n",
    "        closest_negative_item_distances = tf.reduce_min(distance_to_neg_items) #distance_to_neg_items.min(1)[0]\n",
    "\n",
    "        # compute hinge loss (N)\n",
    "        distance = pos_distances - closest_negative_item_distances + self.margin\n",
    "        loss_per_pair = tf.nn.relu(distance) #[]+\n",
    "\n",
    "#         if self.use_rank_weight:\n",
    "#             # indicator matrix for impostors (N x W)\n",
    "#             impostors = (pos_distances - distance_to_neg_items + self.margin) > 0\n",
    "#             # approximate the rank of positive item by (number of impostor / W per user-positive pair)\n",
    "#             rank = impostors.float().mean(1) * self.n_items\n",
    "#             # apply rank weight\n",
    "#             loss_per_pair *= torch.log(rank + 1)\n",
    "\n",
    "        # the embedding loss\n",
    "        loss = tf.reduce_sum(loss_per_pair)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def _feature_projection(self):\n",
    "        if self.features is not None:\n",
    "            output = self.mlp(self.features) * self.feature_projection_scaling_factor\n",
    "            # projection to the embedding\n",
    "            return tf.clip_by_norm(output, self.clip_norm)\n",
    "\n",
    "    def _feature_loss(self):\n",
    "        loss = 0\n",
    "        if feature_projection is not None:\n",
    "            feature_projection = self._feature_projection()\n",
    "            loss = tf.reduce_sum((self.item_embedding.weights - feature_projection) ** 2) * self.feature_l2_reg\n",
    "        return loss\n",
    "\n",
    "    def _covariance_loss(self):\n",
    "        X = tf.concat([self.item_embedding.weights[0], self.user_embedding.weights[0]], 0)\n",
    "        n_rows = X.shape[0]\n",
    "        X -= tf.reduce_mean(X, 0)\n",
    "        cov = tf.matmul(X, X, transpose_a=True) / n_rows\n",
    "        loss = tf.reduce_sum(cov) - tf.linalg.trace(cov)\n",
    "        return loss * self.cov_loss_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 53378/53378 [00:08<00:00, 6095.17it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>297</td>\n",
       "      <td>473</td>\n",
       "      <td>1</td>\n",
       "      <td>405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>252</td>\n",
       "      <td>464</td>\n",
       "      <td>1</td>\n",
       "      <td>915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>285</td>\n",
       "      <td>1013</td>\n",
       "      <td>1</td>\n",
       "      <td>1462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>199</td>\n",
       "      <td>221</td>\n",
       "      <td>1</td>\n",
       "      <td>943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>121</td>\n",
       "      <td>386</td>\n",
       "      <td>1</td>\n",
       "      <td>964</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating   neg\n",
       "0     297      473       1   405\n",
       "1     252      464       1   915\n",
       "2     285     1013       1  1462\n",
       "3     199      221       1   943\n",
       "4     121      386       1   964"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = make_trpilet(train, df['movieId'].unique())\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_user = df['userId'].unique().max()+1\n",
    "n_item = df['movieId'].unique().max()+1\n",
    "\n",
    "model = CML(n_user, n_item, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1669/1669 [==============================] - 4s 2ms/step - loss: 12.0684\n",
      "Epoch 2/10\n",
      "1669/1669 [==============================] - 4s 2ms/step - loss: 2.4378\n",
      "Epoch 3/10\n",
      "1669/1669 [==============================] - 4s 2ms/step - loss: -3.7802\n",
      "Epoch 4/10\n",
      "1669/1669 [==============================] - 4s 2ms/step - loss: -9.6324\n",
      "Epoch 5/10\n",
      "1669/1669 [==============================] - 4s 2ms/step - loss: -15.7213\n",
      "Epoch 6/10\n",
      "1669/1669 [==============================] - 4s 2ms/step - loss: -22.3209\n",
      "Epoch 7/10\n",
      "1669/1669 [==============================] - 4s 2ms/step - loss: -30.1598\n",
      "Epoch 8/10\n",
      "1669/1669 [==============================] - 4s 2ms/step - loss: -38.3810\n",
      "Epoch 9/10\n",
      "1669/1669 [==============================] - 4s 2ms/step - loss: -49.2866\n",
      "Epoch 10/10\n",
      "1669/1669 [==============================] - 4s 2ms/step - loss: -60.2602\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x169de6ac288>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.fit(train.values,\n",
    "         epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 896/896 [00:59<00:00, 15.09it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.031343146472542605"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uiid = df['movieId'].unique()\n",
    "top_k = 10\n",
    "\n",
    "scores = []\n",
    "for user in tqdm(df['userId'].unique()):\n",
    "    user_in = np.full((len(uiid)), user)\n",
    "    inputs = np.dstack([user_in, uiid])[0]\n",
    "    preds = model.predict(inputs)\n",
    "    \n",
    "    item_to_pred = dict(zip(uiid, preds))\n",
    "    test_ = test[(test['userId']==i) & (test['rating']==1)]['movieId'].values\n",
    "    used = train[train['userId']==user]['movieId'].values\n",
    "    items = list(np.random.choice(list(filter(lambda x: x not in used, item_to_pred.keys())), 100)) + list(test_)\n",
    "    top_k_items = heapq.nlargest(top_k, items, key=item_to_pred.get)\n",
    "    \n",
    "    score = eval_NDCG(test_, top_k_items)\n",
    "    scores.append(score)\n",
    "\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
